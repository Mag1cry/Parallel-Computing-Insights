# 实验过程记录

## 实验历程概述

本项目通过四个关键实验，逐步深入理解GPU并行计算的核心范式。

## 实验1：基础GPU加法

### 目标

- 验证CPU/GPU数据传输机制
- 建立基本的CUDA编程模型认知

### 关键代码

```cuda
add_kernel<<<1, 1>>>(a, b, dev_result);
cudaDeviceSynchronize();
cudaMemcpy(&host_result, dev_result, sizeof(int), cudaMemcpyDeviceToHost);
```

### 收获

- 掌握了GPU内存分配(cudaMalloc)
- 理解了内核启动语法(<<< >>>)
- 学会了数据同步(cudaDeviceSynchronize)
- 熟悉了数据传输(cudaMemcpy)

## 实验2：数组并行加法

### 实验2目标

- 体验真正的"子弹齐射"并行模式
- 掌握线程块和网格的组织方式

### 实验2关键代码

```cuda
int tid = threadIdx.x + blockIdx.x * blockDim.x;
if (tid < size) {
    result[tid] = a[tid] + b[tid];
}
```

### 配置参数

- 数组大小：1000个元素
- 线程块大小：256个线程
- 线程块数量：4个块
- 总并行线程：1024个

### 实验2收获

- 体验了真正的并行计算威力
- 掌握了大规模数据并行处理
- 理解了线程索引的计算方式

## 实验3：GPU数值积分（核心实验）

### 实验3目标

- 将数学积分问题映射到硬件并行架构
- 验证分治思想在GPU计算中的自然体现

### 数学问题

计算定积分：∫x²dx 从0到1，解析解为1/3 ≈ 0.333333

### 关键算法

1. **问题分解**：将积分区间[0,1]分为100万份
2. **并行计算**：256个线程同时计算局部面积
3. **结果合并**：使用共享内存和归约操作求和

### 技术亮点

- **共享内存使用**：`__shared__ float shared_sum[256]`
- **线程同步**：`__syncthreads()`确保计算顺序
- **并行归约**：树形结构高效求和

### 精度突破

- 初始版本误差：0.322915（协调不足）
- 修复后误差：0.333333（协调完善）
- 精度提升：0.010418（协调机制的价值）

## 调试历程

### 问题发现

初始实现存在协调问题，导致计算结果偏差较大。

### 调试过程

1. 分析误差模式
2. 检查线程同步机制
3. 优化共享内存使用
4. 改进归约算法

### 关键修复

- 增加线程同步点
- 优化共享内存访问模式
- 改进归约算法实现

## 理论验证

### 范式统一性验证

通过三个实验，验证了：

- **分治策略**：问题分解 → 并行解决 → 结果合并
- **GPU并行**：线程块分解 → 并行执行 → 同步合并  
- **数值积分**：区间分解 → 局部计算 → 求和合并

### 核心洞察

- 并行计算的真正挑战是协调同步，而不仅是同时计算
- 共享内存是GPU线程的"作战会议室"
- 调试过程比最终结果更有教育意义

## 实验总结

### 技术层面

- 掌握了CUDA编程的核心概念
- 理解了GPU并行架构的工作原理
- 学会了并行算法的调试技巧

### 理论层面

- 验证了不同计算范式之间的内在联系
- 建立了从第一性原理出发的探索方法
- 培养了系统性思考问题的能力

### 思维层面

- 从具体实验抽象出通用规律
- 建立了技术探索的自信心
- 培养了持续学习和改进的习惯

## 后续扩展方向

1. **性能优化**：比较不同并行策略的效率
2. **算法扩展**：尝试其他数学问题的GPU实现
3. **应用拓展**：将范式应用到机器学习等领域
4. **理论研究**：深入探索并行计算的理论基础

> 实验不仅是技术的验证，更是思维的训练。通过具体的编码实践，我们获得了对抽象计算范式的深刻理解。
